{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d067045b",
   "metadata": {},
   "source": [
    "**Q1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddf972f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to downloaded dataset: G:\\class codes\\cse440_NLP\\lab 2\\datasets\\lakshmi25npathi\\imdb-dataset-of-50k-movie-reviews\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "\n",
    "# 1. Set the environment variable to your desired path\n",
    "custom_path = \"G:\\class codes\\cse440_NLP\\lab 2\" # Replace with your actual path\n",
    "os.environ[\"KAGGLEHUB_CACHE\"] = custom_path\n",
    "\n",
    "# 2. Download the dataset (it will now use the custom path)\n",
    "# Note: The function will return the path to the downloaded resource within your custom cache.\n",
    "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
    "\n",
    "print(\"Path to downloaded dataset:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e3614b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1/IMDB Dataset.csv\")\n",
    "print(data.head())\n",
    "# print(data['review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ded2ed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "q1_corpus = data['review'].tolist()\n",
    "q1_labels = data['sentiment'].tolist()\n",
    "# print(q1_corpus[0].lower())\n",
    "#preproccess\n",
    "punctutations=['.',',','!','?',';',':','\"',\"'\",'(',')','[',']','{','}','-','_','/','\\\\','@','#','$','%','^','&','*','~','`','<','>','=','+','br']\n",
    "for i in range(len(q1_corpus)):\n",
    "    q1_corpus[i]=word_tokenize(q1_corpus[i].lower())\n",
    "    for word in q1_corpus[i]:\n",
    "        if word in punctutations:\n",
    "            q1_corpus[i].remove(word)\n",
    "\n",
    "def sentence_joiner(tokenized_sentence):\n",
    "    sentence=\"\"\n",
    "    for word in tokenized_sentence:\n",
    "        sentence+=word+\" \"\n",
    "    return sentence.strip()\n",
    "bow=[]\n",
    "for i in range(len(q1_corpus)):\n",
    "    bow.append(sentence_joiner(q1_corpus[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49818547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 101881\n",
      "Dimension:  (50000, 101881)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(bow)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "print(\"Vocabulary Size:\", len(vocab))\n",
    "print(\"Dimension: \",X.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14a50bb",
   "metadata": {},
   "source": [
    "**Q2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ada1f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vocabulary Size: 10000\n",
      "TF-IDF Matrix Shape: (50000, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(bow)\n",
    "vocab_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
    "print(\"TF-IDF Vocabulary Size:\", len(vocab_tfidf))\n",
    "print(\"TF-IDF Matrix Shape:\", X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b0f6939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>007</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>101</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>...</th>\n",
       "      <th>zhang</th>\n",
       "      <th>zizek</th>\n",
       "      <th>zoey</th>\n",
       "      <th>zombi</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zorro</th>\n",
       "      <th>zucco</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.208289</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  007        10  100  1000  101   11   12   13  ...  zhang  zizek  \\\n",
       "0  0.0  0.0  0.0  0.000000  0.0   0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "1  0.0  0.0  0.0  0.000000  0.0   0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "2  0.0  0.0  0.0  0.000000  0.0   0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "3  0.0  0.0  0.0  0.056852  0.0   0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "4  0.0  0.0  0.0  0.000000  0.0   0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "\n",
       "   zoey  zombi    zombie  zombies  zone  zoom  zorro  zucco  \n",
       "0   0.0    0.0  0.000000      0.0   0.0   0.0    0.0    0.0  \n",
       "1   0.0    0.0  0.000000      0.0   0.0   0.0    0.0    0.0  \n",
       "2   0.0    0.0  0.000000      0.0   0.0   0.0    0.0    0.0  \n",
       "3   0.0    0.0  0.208289      0.0   0.0   0.0    0.0    0.0  \n",
       "4   0.0    0.0  0.000000      0.0   0.0   0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 10000 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dframe=pd.DataFrame(X_tfidf.toarray(), columns=vocab_tfidf)\n",
    "dframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2345f269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words by maximum TF-IDF score:\n",
      "1. robot: 0.9157\n",
      "2. ernest: 0.9054\n",
      "3. wine: 0.8780\n",
      "4. woo: 0.8747\n",
      "5. steve: 0.8704\n",
      "6. bad: 0.8675\n",
      "7. custer: 0.8668\n",
      "8. demons: 0.8573\n",
      "9. muppet: 0.8563\n",
      "10. jokes: 0.8530\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "max_scores = np.array(X_tfidf.max(axis=0).todense()).flatten()\n",
    "top_max_indices = max_scores.argsort()[-10:][::-1]\n",
    "\n",
    "print(\"Top 10 words by maximum TF-IDF score:\")\n",
    "for i, idx in enumerate(top_max_indices):\n",
    "    print(f\"{i+1}. {vocab_tfidf[idx]}: {max_scores[idx]:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93257d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88e73a18",
   "metadata": {},
   "source": [
    "# **Q3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc77a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_model(glove_file):\n",
    "    word_vectors = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            word_vectors[word] = vector\n",
    "    return word_vectors\n",
    "glove_file = 'glove.6B.100d.txt'  # Ensure this file is in the working directory\n",
    "glove_model = load_glove_model(glove_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47528288",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher=glove_model['teacher']\n",
    "educate=glove_model['educate']\n",
    "heal=glove_model['heal']\n",
    "doctor=glove_model['doctor']\n",
    "custom=teacher - educate + heal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "deb9d44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'doctor' and the custom(teacher-educate+heal) vector: 0.5419\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "doctor_customer_similarity = cosine_similarity([custom], [doctor])\n",
    "temp=round(float(doctor_customer_similarity[0][0]),4)\n",
    "print(f\"Cosine similarity between 'doctor' and the custom(teacher-educate+heal) vector: {temp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f1ec4",
   "metadata": {},
   "source": [
    "# **Q4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d28f4172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Allen', 'Jr.', '.', '``', 'Only', 'a', 'relative', ...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\nooba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "brown.fileids()\n",
    "brown_text= brown.words('ca01')\n",
    "print(brown_text[100:500])\n",
    "lowered_brown_text = [word.lower() for word in brown_text]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d415f2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\nooba\\anaconda3\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\nooba\\anaconda3\\lib\\site-packages (from gensim) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\nooba\\anaconda3\\lib\\site-packages (from gensim) (1.15.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\nooba\\anaconda3\\lib\\site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\nooba\\anaconda3\\lib\\site-packages (from smart_open>=1.8.1->gensim) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f82dd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_skipgram=Word2Vec(vector_size=100, window=5, sg=1, min_count=1)\n",
    "model_cbow=Word2Vec(vector_size=100, window=5, sg=0, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4d36e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15587, 22420)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_skip=model_skipgram.build_vocab([lowered_brown_text])\n",
    "model_skipgram.train([lowered_brown_text], total_examples=len(lowered_brown_text), epochs=10)\n",
    "vocabulary_cbow=model_cbow.build_vocab([lowered_brown_text])\n",
    "model_cbow.train([lowered_brown_text], total_examples=len(lowered_brown_text), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d55f95c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_vec_skipgram=model_skipgram.wv['teacher']\n",
    "teacher_vec_cbow=model_cbow.wv['teacher']\n",
    "school_vec_skipgram=model_skipgram.wv['school']\n",
    "school_vec_cbow=model_cbow.wv['school']\n",
    "jury_vec_skipgram=model_skipgram.wv['jury']\n",
    "jury_vec_cbow=model_cbow.wv['jury']\n",
    "court_vec_skipgram=model_skipgram.wv['court']\n",
    "court_vec_cbow=model_cbow.wv['court']\n",
    "\n",
    "custom2_skip=teacher_vec_skipgram - school_vec_skipgram + court_vec_skipgram\n",
    "custom2_cbow=teacher_vec_cbow - school_vec_cbow + court_vec_cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ef3bf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'jury' and the custom2_skip(teacher-school+court) vector: 0.9816\n",
      "Cosine similarity between 'jury' and the custom2_cbow(teacher-school+court) vector: 0.1293\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "jury_custom2_skip_similarity = cosine_similarity([custom2_skip], [jury_vec_skipgram])\n",
    "jury_custom2_cbow_similarity = cosine_similarity([custom2_cbow], [jury_vec_cbow])\n",
    "print(f\"Cosine similarity between 'jury' and the custom2_skip(teacher-school+court) vector: {jury_custom2_skip_similarity[0][0]:.4f}\")\n",
    "print(f\"Cosine similarity between 'jury' and the custom2_cbow(teacher-school+court) vector: {jury_custom2_cbow_similarity[0][0]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
