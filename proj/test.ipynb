{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "231b3480",
   "metadata": {},
   "source": [
    "# GPU Support Comparison: Google Colab vs Local Setup\n",
    "\n",
    "## ‚úÖ Google Colab Results (Working)\n",
    "```\n",
    "TensorFlow version: 2.19.0\n",
    "Built with CUDA: True\n",
    "Physical GPUs found: 1\n",
    "  GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
    "Local devices:\n",
    "  /device:CPU:0 CPU 268435456\n",
    "  /device:GPU:0 GPU 14619377664\n",
    "Running a small matmul on /GPU:0 to test execution...\n",
    "GPU matmul completed in 0.2495 s\n",
    "```\n",
    "\n",
    "## ‚ö†Ô∏è Local Setup Status\n",
    "- **Hardware**: NVIDIA GeForce RTX 5080 (15.92 GB VRAM) ‚úÖ\n",
    "- **Driver**: 591.44 (CUDA 13.1 compatible) ‚úÖ\n",
    "- **PyTorch GPU**: Working perfectly (3.18x speedup) ‚úÖ\n",
    "- **TensorFlow GPU**: ‚ùå Not working yet\n",
    "\n",
    "## üîß To Fix TensorFlow GPU Locally:\n",
    "\n",
    "**Current Kernel**: `anaconda3` (Python 3.13.2) - TensorFlow 2.20.0 has **no GPU support**\n",
    "\n",
    "**Solution**: Switch to the **\"TensorFlow GPU (Python 3.11)\"** kernel (top-right corner)\n",
    "- This kernel has TensorFlow 2.16.2 with CUDA packages installed\n",
    "- **BUT** still needs system-level CUDA Toolkit to detect GPU\n",
    "\n",
    "**Next Steps**:\n",
    "1. Download [CUDA Toolkit 12.6](https://developer.nvidia.com/cuda-12-6-0-download-archive)\n",
    "2. Install it (adds necessary DLL files to system PATH)\n",
    "3. Restart VS Code\n",
    "4. Switch kernel to \"TensorFlow GPU (Python 3.11)\"\n",
    "5. Run the TensorFlow test cell below\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56cdaa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "Built with CUDA: False\n",
      "Physical GPUs found: 0\n",
      "Local devices:\n",
      "  /device:CPU:0 CPU 268435456\n",
      "No GPU detected, skipping execution test.\n"
     ]
    }
   ],
   "source": [
    "# Cell (index 0) - quick TensorFlow GPU support check\n",
    "# Put this in the new Notebook cell and run.\n",
    "\n",
    "import time\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except Exception as e:\n",
    "    print(\"Failed to import TensorFlow:\", e)\n",
    "else:\n",
    "    print(\"TensorFlow version:\", tf.__version__)\n",
    "    # Built with CUDA (True/False)\n",
    "    built_with_cuda = False\n",
    "    try:\n",
    "        built_with_cuda = tf.test.is_built_with_cuda()\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(\"Built with CUDA:\", built_with_cuda)\n",
    "\n",
    "    # Physical GPUs detected by TensorFlow\n",
    "    try:\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "    except Exception:\n",
    "        gpus = []\n",
    "    print(\"Physical GPUs found:\", len(gpus))\n",
    "    for i, g in enumerate(gpus):\n",
    "        print(f\"  GPU {i}:\", g)\n",
    "\n",
    "    # Fallback: list local devices (gives CPU/GPU info from TF runtime)\n",
    "    try:\n",
    "        from tensorflow.python.client import device_lib\n",
    "        devices = device_lib.list_local_devices()\n",
    "        print(\"Local devices:\")\n",
    "        for d in devices:\n",
    "            print(\" \", d.name, d.device_type, getattr(d, \"memory_limit\", \"\"))\n",
    "    except Exception as e:\n",
    "        print(\"Could not list local devices:\", e)\n",
    "\n",
    "    # Quick GPU execution test (matrix multiply) if a GPU is available\n",
    "    if gpus:\n",
    "        print(\"Running a small matmul on /GPU:0 to test execution...\")\n",
    "        a = tf.random.uniform((1024, 1024))\n",
    "        b = tf.random.uniform((1024, 1024))\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            with tf.device('/GPU:0'):\n",
    "                c = tf.matmul(a, b)\n",
    "            # force evaluation (works in eager mode)\n",
    "            _ = c.numpy()\n",
    "            t1 = time.time()\n",
    "            print(\"GPU matmul completed in %.4f s\" % (t1 - t0))\n",
    "        except Exception as e:\n",
    "            print(\"GPU test failed:\", e)\n",
    "    else:\n",
    "        print(\"No GPU detected, skipping execution test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3397a1fc",
   "metadata": {},
   "source": [
    "## Test PyTorch GPU Support\n",
    "\n",
    "PyTorch often has better GPU detection on Windows than TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "328dc9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu130\n",
      "CUDA available: True\n",
      "CUDA version: 13.0\n",
      "Number of GPUs: 1\n",
      "\n",
      "GPU 0: NVIDIA GeForce RTX 5080\n",
      "  Memory: 15.92 GB\n",
      "\n",
      "üöÄ Running GPU speed test...\n",
      "‚úÖ GPU matmul (5000x5000) completed in 0.0636 seconds\n",
      "üêå CPU matmul (5000x5000) completed in 0.3469 seconds\n",
      "‚ö° Speedup: 5.46x faster on GPU!\n"
     ]
    }
   ],
   "source": [
    ".import torch\n",
    "import time\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Quick GPU speed test\n",
    "    print(\"\\nüöÄ Running GPU speed test...\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    a = torch.randn(5000, 5000, device=device)\n",
    "    b = torch.randn(5000, 5000, device=device)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    c = torch.matmul(a, b)\n",
    "    torch.cuda.synchronize()\n",
    "    gpu_time = time.time() - start\n",
    "    \n",
    "    print(f\"‚úÖ GPU matmul (5000x5000) completed in {gpu_time:.4f} seconds\")\n",
    "    \n",
    "    # CPU comparison\n",
    "    a_cpu = a.cpu()\n",
    "    b_cpu = b.cpu()\n",
    "    start = time.time()\n",
    "    c_cpu = torch.matmul(a_cpu, b_cpu)\n",
    "    cpu_time = time.time() - start\n",
    "    \n",
    "    print(f\"üêå CPU matmul (5000x5000) completed in {cpu_time:.4f} seconds\")\n",
    "    print(f\"‚ö° Speedup: {cpu_time/gpu_time:.2f}x faster on GPU!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No CUDA GPU detected by PyTorch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
